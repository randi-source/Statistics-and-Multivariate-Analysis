{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler, StandardScaler, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# import mlflow library\n",
    "import mlflow\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models.signature import ModelSignature, infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Kambuno:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21530e20d90>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[*]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"age\", IntegerType(), True),\\\n",
    "    StructField(\"attrition\", StringType(), True),\\\n",
    "    StructField(\"business_travel\", StringType(), True),\\\n",
    "    StructField(\"daily_rate\", IntegerType(), True),\\\n",
    "    StructField(\"department\", StringType(), True),\\\n",
    "    StructField(\"distance_from_home\", IntegerType(), True),\\\n",
    "    StructField(\"education\", IntegerType(), True),\\\n",
    "    StructField(\"education_field\", StringType(), True),\\\n",
    "    StructField(\"employee_count\", IntegerType(), True),\\\n",
    "    StructField(\"employee_number\", IntegerType(), True),\\\n",
    "    StructField(\"employment_satisfaction\", IntegerType(), True),\\\n",
    "    StructField(\"gender\", StringType(), True),\\\n",
    "    StructField(\"hourly_rate\", IntegerType(), True),\\\n",
    "    StructField(\"job_involvement\", IntegerType(), True),\\\n",
    "    StructField(\"job_level\", IntegerType(), True),\\\n",
    "    StructField(\"job_role\", StringType(), True),\\\n",
    "    StructField(\"marital_status\", StringType(), True)\\\n",
    "    StructField(\"monthly_income\", IntegerType(), True)\\\n",
    "    StructField(\"monthly_rate\", IntegerType(), True)\\\n",
    "    StructField(\"num_companies_worked\", IntegerType(), True)\\\n",
    "    StructField(\"over_18\", StringType(), True)\\\n",
    "    StructField(\"over_time\", StringType(), True)\\\n",
    "    StructField(\"percent_salary_hike\", IntegerType(), True)\\\n",
    "    StructField(\"performance_rating\", IntegerType(), True)\\\n",
    "    StructField(\"relationship_satisfaction\", IntegerType(), True)\\\n",
    "    StructField(\"standard_hours\", IntegerType(), True)\\\n",
    "    StructField(\"stock_option_level\", IntegerType(), True)\\\n",
    "    StructField(\"total_working_years\", IntegerType(), True)\\\n",
    "    StructField(\"training_times_last_year\", IntegerType(), True)\\\n",
    "    StructField(\"work_life_balance\", IntegerType(), True)\\\n",
    "    StructField(\"years_at_company\", IntegerType(), True)\\\n",
    "    StructField(\"years_in_current_role\", IntegerType(), True)\\\n",
    "    StructField(\"years_since_last_promotion\", IntegerType(), True)\\\n",
    "    StructField(\"years_with_curr_manager\", IntegerType(), True)\\\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1005</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "5   32        No  Travel_Frequently       1005  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "5                 2          2  Life Sciences              1               8   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "5  ...                         3            80                 0   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "5                  8                      2               2               7   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "5                  7                        3                     6  \n",
       "\n",
       "[6 rows x 35 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Attrition: string (nullable = true)\n",
      " |-- BusinessTravel: string (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EducationField: string (nullable = true)\n",
      " |-- EmployeeCount: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobRole: string (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- Over18: string (nullable = true)\n",
      " |-- OverTime: string (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StandardHours: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Attrition|count|\n",
      "+---------+-----+\n",
      "|       No| 1233|\n",
      "|      Yes|  237|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Attrition\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 5\n",
      "+---------+-----+\n",
      "|Attrition|count|\n",
      "+---------+-----+\n",
      "|       No|  241|\n",
      "|      Yes|  237|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Under Sampling\n",
    "major_df = df.filter(col(\"Attrition\")==\"No\")\n",
    "minor_df = df.filter(col(\"Attrition\")==\"Yes\")\n",
    "\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))\n",
    "\n",
    "sampled_majority_df = major_df.sample(False, 1/ratio)\n",
    "df = sampled_majority_df.unionAll(minor_df)\n",
    "df.groupBy(\"Attrition\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Over Sampling\n",
    "major_df = df.filter(col(\"Attrition\")==\"No\")\n",
    "minor_df = df.filter(col(\"Attrition\")==\"Yes\")\n",
    "\n",
    "ratio = int(major_df.count()/minor_df.count())\n",
    "print(\"ratio: {}\".format(ratio))\n",
    "\n",
    "a = range(ratio)\n",
    "\n",
    "oversampled_df = minor_df.withColumn(\"dummy\", explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "# combine both oversampled minority rows and previous majority rows \n",
    "df = major_df.unionAll(oversampled_df)\n",
    "df.groupBy(\"Attrition\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [\"Age\",\"DailyRate\",\"DistanceFromHome\",\"HourlyRate\",\"JobInvolvement\",\"MonthlyIncome\",\"MonthlyRate\",\"NumCompaniesWorked\",\"PercentSalaryHike\",\"StockOptionLevel\",\"TotalWorkingYears\",\"TrainingTimesLastYear\",\"YearsAtCompany\",\"YearsInCurrentRole\",\"YearsSinceLastPromotion\",\"YearsWithCurrManager\"]\n",
    "col_list_fs = [\"Age\",\"DailyRate\",\"DistanceFromHome\",\"JobInvolvement\",\"MonthlyIncome\",\"NumCompaniesWorked\",\"PercentSalaryHike\",\"StockOptionLevel\",\"TotalWorkingYears\",\"TrainingTimesLastYear\",\"YearsInCurrentRole\",\"YearsSinceLastPromotion\",\"YearsWithCurrManager\"]\n",
    "input_columns = col_list\n",
    "#input_columns\n",
    "#df.limit(6).toPandas()\n",
    "\n",
    "dependent_var = \"Attrition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/16 03:04:07 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '7d62aa42acba488f8e8fcb23b5181b6e', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    }
   ],
   "source": [
    "# change label (class variable) to string type to prep for reindexing\n",
    "# Pyspark is expecting a zero indexed integer for the label column. \n",
    "# Just in case our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \n",
    "indexed = indexer.fit(renamed).transform(renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data in the input column list to numeric\n",
    "# Otherwise the Algorithm will not be able to process it\n",
    "\n",
    "# Also we will use these lists later on\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    # First identify the string vars in your input column list\n",
    "    if str(indexed.schema[column].dataType) == 'StringType':\n",
    "        # Set up your String Indexer function\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "        # Then call on the indexer you created here\n",
    "        indexed = indexer.fit(indexed).transform(indexed)\n",
    "        # Rename the column to a new name so you can disinguish it from the original\n",
    "        new_col_name = column+\"_num\"\n",
    "        # Add the new column name to the string inputs list\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        # If no change was needed, take no action \n",
    "        # And add the numeric var to the num list\n",
    "        numeric_inputs.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MonthlyIncome has been treated for positive (right) skewness. (skew =) 1.5239105561644637 )\n",
      "StockOptionLevel has been treated for positive (right) skewness. (skew =) 1.2560670945509036 )\n",
      "TotalWorkingYears has been treated for positive (right) skewness. (skew =) 1.2796925402059103 )\n",
      "YearsAtCompany has been treated for positive (right) skewness. (skew =) 1.9805957824792155 )\n",
      "YearsInCurrentRole has been treated for positive (right) skewness. (skew =) 1.0671915805104129 )\n",
      "YearsSinceLastPromotion has been treated for positive (right) skewness. (skew =) 1.975622411574525 )\n"
     ]
    }
   ],
   "source": [
    "# Treat for skewness\n",
    "# Flooring and capping\n",
    "# Plus if right skew take the log +1\n",
    "# if left skew do exp transformation\n",
    "# This is best practice\n",
    "\n",
    "# create empty dictionary d\n",
    "d = {}\n",
    "# Create a dictionary of quantiles from your numeric cols\n",
    "# I'm doing the top and bottom 1% but you can adjust if needed\n",
    "for col in numeric_inputs: \n",
    "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "\n",
    "#Now check for skewness for all numeric cols\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # If skewness is found,\n",
    "    # This function will make the appropriate corrections\n",
    "    if skew > 1: # If right skew, floor, cap and log(x+1)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        log(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] ) +1).alias(col))\n",
    "        print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "    elif skew < -1: # If left skew floor, cap and exp(x)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] )).alias(col))\n",
    "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative values were found in your dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Now check for negative values in the dataframe. \n",
    "# Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
    "# Note: we only need to check the numeric input values since anything that is indexed won't have negative values\n",
    "\n",
    "# Calculate the mins for all columns in the df\n",
    "minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) \n",
    "# Create an array for all mins and select only the input cols\n",
    "min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) \n",
    "# Collect golobal min as Python object\n",
    "df_minimum = min_array.select(array_min(min_array.mins)).collect() \n",
    "# Slice to get the number itself\n",
    "df_minimum = df_minimum[0][0] \n",
    "\n",
    "# If there are ANY negative vals found in the df, print a warning message\n",
    "if df_minimum < 0:\n",
    "    print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
    "else:\n",
    "    print(\"No negative values were found in your dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we correct for negative values that may have been found above, \n",
    "# We need to vectorize our df\n",
    "# becauase the function that we use to make that correction requires a vector. \n",
    "# Now create your final features list\n",
    "features_list = numeric_inputs + string_inputs\n",
    "# Create your vector assembler object\n",
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "# And call on the vector assembler to transform your dataframe\n",
    "output = assembler.transform(indexed).select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[49.0,279.0,8.0,6...|  0.0|\n",
      "|[32.0,1005.0,2.0,...|  0.0|\n",
      "|[38.0,371.0,2.0,4...|  0.0|\n",
      "|[24.0,673.0,11.0,...|  0.0|\n",
      "|[53.0,1282.0,5.0,...|  0.0|\n",
      "|[30.0,125.0,9.0,8...|  0.0|\n",
      "|[35.0,1229.0,8.0,...|  0.0|\n",
      "|[26.0,1443.0,23.0...|  0.0|\n",
      "|[37.0,1115.0,1.0,...|  0.0|\n",
      "|[36.0,1223.0,8.0,...|  0.0|\n",
      "|[32.0,548.0,1.0,6...|  0.0|\n",
      "|[36.0,132.0,6.0,5...|  0.0|\n",
      "|[35.0,776.0,1.0,3...|  0.0|\n",
      "|[46.0,945.0,5.0,8...|  0.0|\n",
      "|[23.0,541.0,2.0,6...|  0.0|\n",
      "|[32.0,1093.0,6.0,...|  0.0|\n",
      "|[24.0,1353.0,3.0,...|  0.0|\n",
      "|[58.0,682.0,10.0,...|  0.0|\n",
      "|[32.0,827.0,1.0,7...|  0.0|\n",
      "|[37.0,1040.0,2.0,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/16 03:04:11 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'bd78db57de47446e8d0261e00b77aafa', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current pyspark.ml workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1000.000000]\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[738.095238095238...|\n",
      "|  0.0|[333.333333333333...|\n",
      "|  0.0|[476.190476190476...|\n",
      "|  0.0|[142.857142857142...|\n",
      "|  0.0|[833.333333333333...|\n",
      "|  0.0|[285.714285714285...|\n",
      "|  0.0|[404.761904761904...|\n",
      "|  0.0|[190.476190476190...|\n",
      "|  0.0|[452.380952380952...|\n",
      "|  0.0|[428.571428571428...|\n",
      "|  0.0|[333.333333333333...|\n",
      "|  0.0|[428.571428571428...|\n",
      "|  0.0|[404.761904761904...|\n",
      "|  0.0|[666.666666666666...|\n",
      "|  0.0|[119.047619047619...|\n",
      "|  0.0|[333.333333333333...|\n",
      "|  0.0|[142.857142857142...|\n",
      "|  0.0|[952.380952380952...|\n",
      "|  0.0|[333.333333333333...|\n",
      "|  0.0|[452.380952380952...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the mix max scaler object \n",
    "# This is what will correct for negative values\n",
    "# I like to use a high range like 1,000 \n",
    "#     because I only see one decimal place in the final_data.show() call\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=1000)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "\n",
    "# Compute summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# rescale each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(output)\n",
    "final_data = scaled_data.select('label','scaledFeatures')\n",
    "# Rename to default value\n",
    "final_data = final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First - Read in dependencies\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our evaluation objects\n",
    "Bin_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction') #labelCol='label'\n",
    "# Bin_evaluator = BinaryClassificationEvaluator() #labelCol='label'\n",
    "MC_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\") # redictionCol=\"prediction\",\n",
    "MC_evaluator_acc = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='accuracy')\n",
    "MC_evaluator_f1 = MulticlassClassificationEvaluator(predictionCol='prediction', labelCol='label', metricName='f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8001/\")\n",
    "\n",
    "mlflow.set_experiment(experiment_name = \"employee_attrition_log_model\")\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/16 03:04:22 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: INVALID_PARAMETER_VALUE: Invalid value [{'key': 'collectSubModels', 'value': 'False'}, {'key': 'estimator', 'value': 'LogisticRegression'}, {'key': 'evaluator', 'value': 'MulticlassClassificationEvaluator'}, {'key': 'MulticlassClassificationEvaluator.beta', 'value': '1.0'}, {'key': 'MulticlassClassificationEvaluator.eps', 'value': '1e-15'}, {'key': 'MulticlassClassificationEvaluator.labelCol', 'value': 'label'}, {'key': 'MulticlassClassificationEvaluator.metricLabel', 'value': '0.0'}, {'key': 'MulticlassClassificationEvaluator.metricName', 'value': 'accuracy'}, {'key': 'MulticlassClassificationEvaluator.predictionCol', 'value': 'prediction'}, {'key': 'MulticlassClassificationEvaluator.probabilityCol', 'value': 'probability'}, {'key': 'foldCol', 'value': ''}, {'key': 'numFolds', 'value': '3'}, {'key': 'parallelism', 'value': '1'}, {'key': 'seed', 'value': '-2585470059139936778'}] for parameter 'params' supplied. Hint: Value was of type 'list'. See the API docs for more information about request parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: [3.805882116296268]\n",
      "Coefficients: \n",
      "DenseMatrix([[-8.41700043e-04, -5.49173546e-04,  1.56483716e-03,\n",
      "               1.21430249e-04, -1.80879249e-03, -1.52565485e-03,\n",
      "              -1.92388482e-04,  1.29247454e-03,  5.23021079e-05,\n",
      "              -1.26579340e-03, -7.94031107e-04, -1.15757783e-03,\n",
      "              -2.18807040e-03, -7.51426334e-04,  1.74983546e-03,\n",
      "              -6.44618010e-04]])\n",
      "accuracy: 67.17557251908397\n",
      "f1 score: 0.672176624768027\n",
      "auc: 0.676906779661017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.73      0.67        59\n",
      "         1.0       0.74      0.62      0.68        72\n",
      "\n",
      "    accuracy                           0.67       131\n",
      "   macro avg       0.68      0.68      0.67       131\n",
      "weighted avg       0.68      0.67      0.67       131\n",
      "\n",
      "[[43 16]\n",
      " [27 45]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/16 03:04:33 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\user\\AppData\\Local\\Temp\\tmpntd7vtmb\\model, flavor: spark), fall back to return ['pyspark==3.3.0']. Set logging level to DEBUG to see the full traceback.\n",
      "Registered model 'employee_attrition_logreg' already exists. Creating a new version of this model...\n",
      "2022/10/16 03:04:33 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: employee_attrition_logreg, version 14\n",
      "Created version '14' of model 'employee_attrition_logreg'.\n"
     ]
    }
   ],
   "source": [
    "# First tell Spark which classifier you want to use\n",
    "mlflow.pyspark.ml.autolog()\n",
    "\n",
    "with mlflow.start_run(run_name=\"logistic_regression\") as run:\n",
    "    classifier = LogisticRegression()\n",
    "\n",
    "    # Then Set up your parameter grid for the cross validator to conduct hyperparameter tuning\n",
    "    paramGrid = (ParamGridBuilder() \\\n",
    "                .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "                .addGrid(classifier.maxIter, [10, 15,20])\n",
    "                .build())\n",
    "\n",
    "    # Then set up the Cross Validator which requires all of the following parameters:\n",
    "    crossval = CrossValidator(estimator=classifier,\n",
    "                            estimatorParamMaps=paramGrid,\n",
    "                            evaluator=MC_evaluator,\n",
    "                            numFolds=3) # 3 + is best practice, but its just taking more times to train\n",
    "\n",
    "    # Then fit your model\n",
    "    fitModel = crossval.fit(train)\n",
    "\n",
    "    # Collect the best model and\n",
    "    # print the coefficient matrix\n",
    "    # These values should be compared relative to eachother\n",
    "    # And intercepts can be prepared to other models\n",
    "    BestModel = fitModel.bestModel\n",
    "    print(\"Intercept: \" + str(BestModel.interceptVector))\n",
    "    print(\"Coefficients: \\n\" + str(BestModel.coefficientMatrix))\n",
    "\n",
    "    # You can extract the best model from this run like this if you want\n",
    "    LR_BestModel = BestModel\n",
    "\n",
    "    # Next you need to generate predictions on the test dataset\n",
    "    # fitModel automatically uses the best model \n",
    "    # so we don't need to use BestModel here\n",
    "    predictions = fitModel.transform(test)\n",
    "\n",
    "    # Now print the accuracy rate of the model or AUC for a binary classifier\n",
    "    accuracy = (MC_evaluator_acc.evaluate(predictions))*100\n",
    "    print(\"accuracy:\", accuracy)\n",
    "    f1_score = (MC_evaluator_f1.evaluate(predictions))\n",
    "    print(\"f1 score:\", f1_score)\n",
    "    #weighted_precision = MC_evaluator_prec.evaluate(predictions)\n",
    "    #print(\"weighted precision:\", weighted_precision)\n",
    "    #weighted_recall = MC_evaluator_rec.evaluate(predictions)\n",
    "    #print(\"weighted recall:\", weighted_recall)\n",
    "    #weighted_true_positive_rate = MC_evaluator_wtpr.evaluate(predictions)\n",
    "    #print(\"weighted true positive rate:\", weighted_true_positive_rate)\n",
    "    #weighted_false_positice_rate = MC_evaluator_wfpr.evaluate(predictions)\n",
    "    #print(\"weighted false positive rate:\", weighted_false_positive_rate)\n",
    "    auc = (Bin_evaluator.evaluate(predictions))\n",
    "    print(\"auc:\", auc)\n",
    "\n",
    "    # Load the Summary\n",
    "    trainingSummary = LR_BestModel.summary\n",
    "\n",
    "    accuracy = trainingSummary.accuracy\n",
    "    falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "    truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "    fMeasure = trainingSummary.weightedFMeasure()\n",
    "    precision = trainingSummary.weightedPrecision\n",
    "    recall = trainingSummary.weightedRecall\n",
    "\n",
    "    ########### Track results in MLflow UI ################\n",
    "\n",
    "    y_true = predictions.select(['label']).collect()\n",
    "    y_pred = predictions.select(['prediction']).collect()\n",
    "    y_pred_proba = predictions.select(['probability']).collect()\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    conf_matrix= confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    mlflow.spark.log_model(artifact_path = \"model\",\n",
    "            spark_model = BestModel,\n",
    "            input_example = df.limit(4).toPandas(),\n",
    "            #code_paths = 'D:\\testcase08\\ready monitoring\\atp_small_user_log.ipynb',\n",
    "            #signature = infer_signature(test, fitModel.transform(test)),\n",
    "            #signature = ModelSignature(inputs = input_schema, outputs = output_schema),\n",
    "            registered_model_name = \"employee_attrition_logreg\")\n",
    "\n",
    "    # Extract params of Best Model\n",
    "    paramMap = BestModel.extractParamMap()\n",
    "\n",
    "    # Log parameters to the client\n",
    "    for key, val in paramMap.items():\n",
    "        if 'maxIter' in key.name:\n",
    "            client.log_param(run.info.run_id, \"Max Iter\", val)\n",
    "    for key, val in paramMap.items():\n",
    "        if 'regParam' in key.name:\n",
    "            client.log_param(run.info.run_id, \"Reg Param\", val)\n",
    "\n",
    "    #client.log_artifact(run.info.run_id, \"Confusion Matrix\", conf_matrix)\n",
    "\n",
    "    mlflow.log_artifact(\"tugbes_paper_baru.ipynb\")\n",
    "    #mlflow.log_artifact(conf_matrix)\n",
    "\n",
    "    # Log metrics to the client\n",
    "    client.log_metric(run.info.run_id, \"Accuracy\", accuracy)\n",
    "    client.log_metric(run.info.run_id, \"F1 Score\", f1_score)\n",
    "    client.log_metric(run.info.run_id, \"AUC\", auc)\n",
    "    #client.log_metric(run.info.run_id, \"Weighted Precision\", weighted_precision)\n",
    "    #client.log_metric(run.info.run_id, \"Weighted Recall\", weighted_recall)\n",
    "    #client.log_metric(run.info.run_id, \"Weighted True Positive Rate\", weighted_true_positive_rate)\n",
    "    #client.log_metric(run.info.run_id, \"Weighted False Positive Rate\", weighted_false_positive_rate\n",
    "\n",
    "    #test_1 = test.toPandas()\n",
    "\n",
    "    #mlflow.whylogs.log_pandas(test_1)\n",
    "\n",
    "    # Set a runs status to finished (best practice)\n",
    "    client.set_terminated(run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'DailyRate',\n",
       " 'DistanceFromHome',\n",
       " 'HourlyRate',\n",
       " 'JobInvolvement',\n",
       " 'MonthlyIncome',\n",
       " 'MonthlyRate',\n",
       " 'NumCompaniesWorked',\n",
       " 'PercentSalaryHike',\n",
       " 'StockOptionLevel',\n",
       " 'TotalWorkingYears',\n",
       " 'TrainingTimesLastYear',\n",
       " 'YearsAtCompany',\n",
       " 'YearsInCurrentRole',\n",
       " 'YearsSinceLastPromotion',\n",
       " 'YearsWithCurrManager']"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip input_columns qith feature importance scores and create df\n",
    "\n",
    "# First convert featureimportance scores from numpy array to list\n",
    "coeff_array = BestModel.coefficientMatrix.toArray()\n",
    "coeff_scores = []\n",
    "for x in coeff_array[0]:\n",
    "    coeff_scores.append(float(x))\n",
    "# Then zip with input_columns list and create a df\n",
    "\n",
    "#data_schema = [StructField(\"feature\", StringType(), True),StructField(\"coeff\", FloatType(), True)]\n",
    "#final_struc = StructType(fields=data_schema)\n",
    "#column = input_columns\n",
    "#result = spark.createDataFrame(zip(input_columns,coeff_scores), column)\n",
    "\n",
    "input_columns\n",
    "#result.show()\n",
    "#result = spark.createDataFrame(zip(input_columns,coeff_scores), schema=['feature','coeff'])\n",
    "#result.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0008417000425776801,\n",
       " -0.0005491735462164814,\n",
       " 0.001564837163566775,\n",
       " 0.00012143024934070559,\n",
       " -0.0018087924907009979,\n",
       " -0.0015256548497179165,\n",
       " -0.00019238848217411914,\n",
       " 0.0012924745350110707,\n",
       " 5.230210785524785e-05,\n",
       " -0.0012657934021481046,\n",
       " -0.0007940311072291798,\n",
       " -0.0011575778252562918,\n",
       " -0.002188070396149016,\n",
       " -0.0007514263343524888,\n",
       " 0.001749835461896528,\n",
       " -0.0006446180099079643]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coeff_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-------------------+\n",
      "|summary|             label|         prediction|\n",
      "+-------+------------------+-------------------+\n",
      "|  count|               347|                347|\n",
      "|   mean|0.4755043227665706| 0.4495677233429395|\n",
      "| stddev|0.5001207563529201|0.49816842324132116|\n",
      "|    min|               0.0|                0.0|\n",
      "|    max|               1.0|                1.0|\n",
      "+-------+------------------+-------------------+\n",
      "\n",
      " \n",
      "objectiveHistory: (scaled loss + regularization) at each iteration\n",
      "0.6919466236310897\n",
      "0.651042093567163\n",
      "0.5754645277661793\n",
      "0.5519654264709095\n",
      "0.5437304767618284\n",
      "0.5404854443624808\n",
      "0.5387722675779191\n",
      "0.5384635579913163\n",
      "0.5383951539956239\n",
      "0.5383521873504922\n",
      "0.5383271294446412\n",
      " \n",
      "False positive rate by label:\n",
      "label 0: 0.2909090909090909\n",
      "label 1: 0.21428571428571427\n",
      " \n",
      "True positive rate by label:\n",
      "label 0: 0.7857142857142857\n",
      "label 1: 0.7090909090909091\n",
      " \n",
      "Precision by label:\n",
      "label 0: 0.7486910994764397\n",
      "label 1: 0.75\n",
      " \n",
      "Recall by label:\n",
      "label 0: 0.7857142857142857\n",
      "label 1: 0.7090909090909091\n",
      " \n",
      "F-measure by label:\n",
      "label 0: 0.7667560321715817\n",
      "label 1: 0.7289719626168225\n",
      " \n",
      "Accuracy: 0.7492795389048992\n",
      "FPR: 0.2544743440997043\n",
      "TPR: 0.7492795389048992\n",
      "F-measure: 0.748789543766581\n",
      "Precision: 0.7493134873334641\n",
      "Recall: 0.7492795389048992\n"
     ]
    }
   ],
   "source": [
    "# Load the Summary\n",
    "trainingSummary = LR_BestModel.summary\n",
    "\n",
    "# General Describe\n",
    "trainingSummary.predictions.describe().show()\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\" \")\n",
    "print(\"objectiveHistory: (scaled loss + regularization) at each iteration\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# for multiclass, we can inspect metrics on a per-label basis\n",
    "print(\" \")\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\" \")\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\" \")\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\" \")\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\" \")\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))\n",
    "\n",
    "# Generate confusion matrix and print (includes accuracy)\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\" \")\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.ml.classification.BinaryLogisticRegressionTrainingSummary at 0x2153136d460>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_BestModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlflow_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b41cfe3ec2f3e6d518910cc3f67fc30daa273c36860182e6aea30afe72e33e74"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
